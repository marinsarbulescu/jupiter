So what if you want to read a little
bit more about the discussion of the differences between machinery and
traditional statistics. Well, I am going to give you a couple of
articles that you can go get and read. One is a blog post by Larry Wasserman. He is a well known statistician, and he wrote a blog post if you search
Called the Rise of the Machines. It's a very good treatment
of the distinction and maybe a little bit of a rallying
cry to the statistical community to embrace machine learning
a little bit better. There was this very celebrated
statistician who sadly passed away, named Leo Breiman. And he wrote a tremendously
statistical paper called Statistical Modeling:
The Two Cultures, and much of my thinking really
derives from this paper. It's very accessible. It's in the Journal of
Statistical Science, and I would say his approach is quite
critical of traditional statistics, or at least suggesting that traditional
statistics is lagging behind. I give you this quote here, where he says,
in this paper I will argue that the focus in the statistical community on data
models has led to irrelevant theory. Kept statisticians from using more
suitable algorithmic models and prevented statisticians from
working on exciting new problems. So, he kind of takes his own
community to task quite a bit. It's a very entertaining article. And it's not quite as dry and
academic as most scientific, most statistical research papers, and I just brought out this wonderful quote
that really stuck with me from D.R. Cox. D.R. Cox is another tremendously
celebrated statistician, his Cox proportional hazard
model is easily one of the top five most cited statistical papers. And Dr. Cox was one of the discussants
of the paper, and this quote really stuck with me, where he said, Professor
Breiman takes data as his starting point. I would prefer to start with an issue,
a question, or a scientific hypothesis. Although I would be surprised if this
were a real source of disagreement. But I really like this comment because
I do think that really does get at to me, I think of the things I have the most
trouble with with machine learning. It's when you're having to both derive and interrogate the hypothesis
with the same set of data. I think to me,
that's the hardest pill to swallow. And then the final article that you
might be interested in looking at was an article by David Hand,
another great statistician. And he says Classifier Technology and
the Illusion of Progress, so this one's a little bit more critical
of the machine learning approach, and I'll give you a quote from this. It says, so that, so that the apparent superiority of more sophisticated
methods may be something of an illusion. In particular simpler methods typically
yield performance almost as good as more sophisticated methods, to the extent that
the difference in performance may be swamped by other sources of uncertainty
that generally are not considered in the classical supervised
classification paradigm. So he's basically saying the effort
of building a complicated machine learning algorithm
is often not worth it. The increase in complexity
leads to marginal gains is one of the points that he is making. So it's another interesting paper, quite
the opposite of say Leo Breiman's paper. So that gives you, hopefully, enough
to go on for the rest of the course. We're really gonna kinda focus
on more traditional analysis, under the assumption that
you're going to be using and managing data scientists to try and
come up with parsimonious explanations for your industry or
whatever it is you're trying to study. So thanks for listening and I look forward
to seeing you in the next lecture.