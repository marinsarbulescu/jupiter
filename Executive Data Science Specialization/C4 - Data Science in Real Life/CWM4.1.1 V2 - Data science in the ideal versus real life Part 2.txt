Hi, my name is Brian Caffo, and this is lecture one in week four of
the Executive Data Science Series. In this lecture, we're going to talk about
the perfect data science experience, in contrast with what actually
happens in real life. So in the perfect data science experience,
what happens? We have clearly defined hypotheses of
interest that you've specified a priori. Experimental design is at your disposal so you have a lot of control over what
you do prior to collecting data. So for example, you might be
able to conduct an AB test and do randomization across
a treatment of interest. Or if you have some nuisance variables
that you're not interested in, you might be able to
stratify across those. And randomization isn't enough by itself. You actually have a random sample from
a population that you're interested in. So when you draw your conclusions from
your sample, you know that they will, hopefully be representative to
the population that you're really interested in. The data that you measure is actually able
to interrogate the hypothesis that you're interested in. That's an important other aspect that
we'll talk a little bit more about later. And then your data creation,
creating your database, whatever merging you have to do,
all of that goes smoothly. There's no missing data,
there's no failed entries, there's no non-unique matches so
all of that goes perfectly. Analysis is robust without the need for
some sort of advanced modeling. So you can get by with a simple t test or
something like that and then your conclusions are clear. You gain parsimonious knowledge
from the experiment and your decision is obvious given the data. So that's the perfect scenario. What actually happens? So what typically happens is you don't have strongly specified
operatory hypothesis and your data's actually needed to inform your hypotheses
before you start to interrogate them. And usually you're having to
use the same data set tocome up with the hypothesis as you are to
decide between the hypotheses. And because of that and because you're
often looking at multiple hypotheses, multiple comparisons are an issue. Your access to experimental
design is often limited, or the data is just completely observational
so randomization isn't available, or even in a setting that comes up commonly in
biostatistics, your data is retrospective. So, you just look at the outcomes and look
backwards to see what covariates they had. Often, the population being studied
isn't the population of interest. So, if you're Google and if you happen to be analyzing search
data from Yahoo, then that's not ideal. But if that's an instance where that's
the only data you happen to have, then that's the data
that you happen to have. Another problem is that you don't have
the exact measurements that you need to evaluate the hypotheses. And that's a very common problem,
in my experience. The data is problematic, you have some issue with merging
where you have non-unique matches, or multiple matches, you have data
entry errors, you have missing data. And then, because of all these errors and
intricacies in the observational nature of the data,
advanced modeling was being required. Because advanced modeling is required, advanced computing is needed
to fit your advanced models. And because of that, there's issues
of both computational robustness, how fragile the model is to the
assumptions that you've made, and so on. And then,
your conclusions might be indeterminate. And then your decision space might
not be substantially more informed by the data than if you
had done nothing at all.